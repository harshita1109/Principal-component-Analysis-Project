# -*- coding: utf-8 -*-
"""PCA Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17BuXrzsKMCy_irJcmPy_2j85rIfpBtsq

PCA
1. REDUCE THE DIMENSION.

# STEPS:
1. Standardise the range of continuous initial variable.
2. Co variance matrix to identify the correlation between matrix.
3. Eigen value and eigen vector to identify the principal components.
4. Sort the eigen values in descending order and pick the highest one.
"""

from sklearn import datasets

from sklearn.datasets import load_digits

dataset=load_digits()

dataset

dataset.keys()

import numpy as np
import pandas as pd

df=pd.DataFrame(dataset.data,columns=dataset.feature_names)
df

df['Target']=dataset.target
df

print("Target is ",dataset.target[0])         #this means first image is zero
dataset.data[0]

dataset.data[0].reshape(8,8)

import matplotlib.pyplot as plt

for x in range(5):
  plt.gray()
  plt.matshow(dataset.data[x].reshape(8,8))
  plt.show()

"""**PCA**
1. Prepare the data for PCA.
2. Find how to select the number of components.
"""

X=df.iloc[:,:-1]
Y=dataset.target

X

Y

"""**APPLYING STANDARDIZATION**
# If we apply standardization in data:
    mean=0
    std. dev=1
"""

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)

X_scaled

pd.DataFrame(X_scaled)

X_scaled.std()

X_scaled.mean()

"""**CO VARIANCE MATRIX**
find the relations between features (independent columns) and gain important features . To do that we will now find the co variance matrix

# it helps PCA to understand how the data features are related so it can reduce the dimensions while keeping the most important information.
"""

X_T=X_scaled.T
pd.DataFrame(X_scaled)

cov_mat=np.cov(X_T)
pd.DataFrame(cov_mat)

"""# With the help of co variance matrix we can easily find the eigen values and eigen vectors.

1. Eigen vectors- tells the best angle so that we can capture the max info from it.
2. Eigen values- It tells how much value we will get from the direction of eigen vectors
"""

eig_values,eig_vectors=np.linalg.eig(cov_mat)

pd.DataFrame(eig_values)

pd.DataFrame(eig_vectors)

"""**EXPLAINED VARIANCE**
# It tells how much information or variance each principle component captures.

# More variance means it is explaining principle component ,more imp it is.

**HOW TO CALCULATE**
To find total info = Eigen values/ Sum of eigen values
"""

pd.DataFrame(eig_values)

cumulative_var_exp=np.cumsum(var_exp)
print("Cumulative Variance Explained (first 10 components):\n", cumulative_var_exp[:10])
print("Cumulative Variance Explained (last value): ", cumulative_var_exp[-1])

cumulative_df=pd.DataFrame(cumulative_var_exp)
cumulative_df.head(30)

plt.figure(figsize=(10,5))
plt.bar(range(len(var_exp)), var_exp, label='Individual_explained_variance', color='g')
plt.step(range(len(cumulative_var_exp)), cumulative_var_exp, label='cumulative variance explained')
plt.xlabel('Principal Component Index')
plt.ylabel('Explained variance ratio')
plt.legend()
plt.show()

from sklearn.decomposition import PCA

pca=PCA(0.95)
X_pca=pca.fit_transform(X)

X_pca.shape

manual_pca=PCA(n_components=27)

X_pca_manual=manual_pca.fit_transform(X_scaled)

X_pca_manual.shape

Y

from sklearn.model_selection import train_test_split

X_train,X_test,Y_train,Y_test=train_test_split(X_pca,Y,test_size=0.25,random_state=42)

X_train.shape

Y_train.shape

X_test.shape

Y_test.shape

from sklearn.linear_model import LogisticRegression

model=LogisticRegression(max_iter=1000)
model.fit(X_train,Y_train)

y_pred_manual=model_manual.predict(X_test)
y_pred_manual

from sklearn.metrics import accuracy_score

accuracy_score(Y_test,y_pred_manual)*100

model_manual=LogisticRegression(max_iter=1000)

X_train,X_test,Y_train,Y_test=train_test_split(X_pca_manual,Y,test_size=0.25,random_state=42)

X_train.shape

Y_train.shape

X_test.shape

Y_test.shape

model_manual.fit(X_train,Y_train)

Y_pred=model_manual.predict(X_test)
Y_pred

accuracy_score(Y_pred,Y_test)*100







